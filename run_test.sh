CUDA_VISIBLE_DEVICES=4,5,6,7 nohup torchrun --nnodes 1 --nproc_per_node 4 --master_port 29504 finetuning.py \
	--enable_fsdp  \
	--model_name /data/hfmodel/PLMs/llama27b_hf \
	--peft_method lora \
	--use_peft true \
	--dataset grammar_dataset \
	--save_model \
	--dist_checkpoint_root_folder model_checkpoints \
	--dist_checkpoint_folder fine-tuned \
	--fsdp_config.pure_bf16 \
	--lr 5e-6 \
	--output_dir loras/decisioner-100-epoch60-prompt \
	--train_split ./data/demo_train.json \
	--batch_size_training 128 \
	--lora_path '' \
	--step_size 1 \
	--num_epochs 10  > logs/decisioner-100-epoch60-prompt.log 2>&1 &
